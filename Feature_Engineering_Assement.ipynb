{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnDEkfA5DbkYluIdCObOA1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshi-Data-Art/Cluster/blob/main/Feature_Engineering_Assement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) What is a parameter?\n",
        "\n",
        "In feature engineering, a parameter is a value or setting that controls how a transformation, model, or process is applied to the data. For example, in scaling, a parameter could be the scaling method (e.g., standardization); in feature selection, it could be the number of features to keep. Parameters influence how features are prepared or manipulated before training a machine learning model."
      ],
      "metadata": {
        "id": "4nlFIFY9XzDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) What is correlation?\n",
        "What does negative correlation mean?\n",
        "\n",
        "Correlation is a statistical measure that describes the relationship between two variables. It shows how changes in one variable are associated with changes in another. Correlation values range from -1 to 1:\n",
        "\n",
        "+1: Perfect positive correlation (both variables increase together).\n",
        "\n",
        "0: No correlation (no relationship between the variables).\n",
        "\n",
        "-1: Perfect negative correlation (one variable increases as the other decreases).\n",
        "\n",
        "Negative Correlation:\n",
        "A negative correlation means that as one variable increases, the other decreases. For example, if variable X has a negative correlation with variable Y, then as X increases, Y tends to decrease, and vice versa.\n",
        "\n",
        "In Feature Engineering:\n",
        "Positive Correlation: Features that are positively correlated can sometimes be redundant, and you might choose to remove one to avoid multicollinearity.\n",
        "\n",
        "Negative Correlation: Negative correlation can indicate an inverse relationship, and sometimes it might be useful in modeling, depending on the context.\n",
        "\n",
        "In feature engineering, understanding the correlation between features helps in selecting or transforming features to improve model performance."
      ],
      "metadata": {
        "id": "uZdeEK5vXzAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Machine Learning (ML) is a branch of artificial intelligence where computers learn from data to make predictions or decisions without explicit programming.\n",
        "\n",
        "Main Components:\n",
        "Data: The input used to train models.\n",
        "\n",
        "Model: The mathematical representation that learns patterns from data.\n",
        "\n",
        "Algorithm: The method used to train the model.\n",
        "\n",
        "Features: The variables from the data used for training.\n",
        "\n",
        "Training: The process of learning patterns from data.\n",
        "\n",
        "Evaluation: Assessing model performance using metrics.\n",
        "\n",
        "Prediction: Using the trained model to make predictions on new data."
      ],
      "metadata": {
        "id": "3YdwEgFjXy9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "The loss value measures how well or poorly a model's predictions match the actual values. A low loss indicates that the model's predictions are close to the true values, meaning the model is performing well. A high loss suggests that the model is making poor predictions, signaling that it needs improvement. Monitoring the loss value during training helps in evaluating the model's performance and guiding adjustments to improve accuracy."
      ],
      "metadata": {
        "id": "b4_lyxe-Xy6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) What are continuous and categorical variables?\n",
        "\n",
        "Continuous variables are numeric variables that can take any value within a range, including decimals. Examples include age, height, or temperature. These variables are typically used in regression tasks.\n",
        "\n",
        "Categorical variables are variables that represent categories or groups. They can take a limited number of distinct values or labels, such as color, gender, or type of product. Categorical variables are often used in classification tasks and may need encoding for machine learning models.\n",
        "\n",
        "In feature engineering, continuous variables might be scaled, and categorical variables may be one-hot encoded or label encoded to make them suitable for model training."
      ],
      "metadata": {
        "id": "fKv6JZXVXy4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Handling categorical variables is crucial in machine learning because most algorithms work with numerical data. Here are common techniques to handle categorical variables:\n",
        "\n",
        "1. One-Hot Encoding:\n",
        "This technique converts each category into a new binary feature (0 or 1).\n",
        "\n",
        "For example, if you have a \"color\" feature with values \"red\", \"blue\", and \"green\", one-hot encoding would create three columns, each representing one color.\n",
        "\n",
        "Use case: Works well with nominal categories (no intrinsic order).\n",
        "\n",
        "2. Label Encoding:\n",
        "Each category is assigned a unique integer. For example, \"red\" = 0, \"blue\" = 1, and \"green\" = 2.\n",
        "\n",
        "Use case: Suitable for ordinal categories (where there's a meaningful order).\n",
        "\n",
        "3. Ordinal Encoding:\n",
        "Similar to label encoding but specifically for categorical variables that have a natural order (e.g., \"low\", \"medium\", \"high\").\n",
        "\n",
        "It assigns integers that preserve the order of the categories.\n",
        "\n",
        "4. Frequency or Count Encoding:\n",
        "Categories are replaced by their frequency or count in the dataset. For example, if \"red\" appears 50 times, it would be replaced by 50.\n",
        "\n",
        "Use case: Useful when categories have a high cardinality and there's a relationship between frequency and importance.\n",
        "\n",
        "5. Target Encoding (Mean Encoding):\n",
        "Each category is replaced by the mean of the target variable for that category.\n",
        "\n",
        "Use case: Effective when there's a strong relationship between categorical variables and the target, especially in classification tasks.\n",
        "\n",
        "6. Binary Encoding:\n",
        "Categories are first label-encoded and then converted into binary format.\n",
        "\n",
        "Use case: Useful when there are a large number of categories to reduce the dimensionality.\n",
        "\n",
        "7. Hashing:\n",
        "A technique to map categories to a fixed-size vector using a hash function.\n",
        "\n",
        "Use case: Can handle large categorical features but may cause collisions (multiple categories mapped to the same value).\n",
        "\n",
        "Choosing the technique:\n",
        "For nominal data (no inherent order), one-hot encoding or frequency encoding works well.\n",
        "\n",
        "For ordinal data (with order), label encoding or ordinal encoding is suitable.\n",
        "\n",
        "For high cardinality categorical features, target encoding or hashing can be effective."
      ],
      "metadata": {
        "id": "8a3yZk8BXy1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) What do you mean by training and testing a dataset?\n",
        "\n",
        "Training and testing a dataset refer to the process of splitting data to build and evaluate machine learning models.\n",
        "\n",
        "Training a Dataset:\n",
        "\n",
        "The training dataset is used to train the machine learning model. During training, the model learns patterns and relationships in the data by adjusting its internal parameters.\n",
        "\n",
        "The goal is to minimize errors (or loss) and enable the model to generalize well to new data.\n",
        "\n",
        "Testing a Dataset:\n",
        "\n",
        "The testing dataset is a separate set of data that the model has not seen during training. After the model is trained, it is evaluated on this dataset to assess how well it generalizes to new, unseen data.\n",
        "\n",
        "Testing helps estimate the model's performance (e.g., accuracy, precision, recall) and check for issues like overfitting."
      ],
      "metadata": {
        "id": "6W-0NV3ZXyzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module in Scikit-learn that provides tools to preprocess data for machine learning models. It includes methods for:\n",
        "\n",
        "Scaling/Normalization: Tools like StandardScaler and MinMaxScaler to scale data.\n",
        "\n",
        "Encoding: Methods like OneHotEncoder and LabelEncoder to handle categorical variables.\n",
        "\n",
        "Imputation: Filling missing values using SimpleImputer.\n",
        "\n",
        "Binarization: Converting data into binary format with Binarizer.\n",
        "\n",
        "Polynomial Features: Creating polynomial features with PolynomialFeatures."
      ],
      "metadata": {
        "id": "hs5EQwByXyvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) What is a Test set?\n",
        "\n",
        "A test set is a subset of the data that is used to evaluate the performance of a trained machine learning model. It contains data that the model has not seen during training, ensuring an unbiased assessment of how well the model generalizes to new, unseen data. In feature engineering, the test set is kept separate from the training set to avoid data leakage and overfitting."
      ],
      "metadata": {
        "id": "1nwMvr6dXytC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "Splitting Data in Python:\n",
        "Use train_test_split from Scikit-learn to split data into training and testing sets:\n",
        "\n",
        "Approach to Machine Learning:\n",
        "Define the Problem: Identify if it's classification, regression, etc.\n",
        "\n",
        "Prepare Data: Clean, preprocess, and split into training and testing sets.\n",
        "\n",
        "Choose Model: Select an appropriate model (e.g., KNN, Decision Tree).\n",
        "\n",
        "Train Model: Fit the model on training data.\n",
        "\n",
        "Evaluate Model: Test performance on the testing set.\n",
        "\n",
        "Tune Model: Optimize using hyperparameters and cross-validation.\n",
        "\n",
        "Deploy: Deploy the model for real-world use."
      ],
      "metadata": {
        "id": "sHYvpYgVXyqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11) Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "EDA (Exploratory Data Analysis) is performed before fitting a model to:\n",
        "\n",
        "Understand data structure and relationships.\n",
        "\n",
        "Identify missing values, outliers, or anomalies.\n",
        "\n",
        "Choose the right features and model.\n",
        "\n",
        "Clean data by fixing issues like duplicates or incorrect types.\n",
        "\n",
        "Improve model performance through insights on data patterns.\n",
        "\n",
        "It helps prepare data for accurate and efficient modeling."
      ],
      "metadata": {
        "id": "accgKgwMXynk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12) What is correlation?\n",
        "\n",
        "Correlation is a statistical measure that describes the relationship between two variables. It indicates how changes in one variable are associated with changes in another.\n",
        "\n",
        "Positive correlation means both variables increase or decrease together.\n",
        "\n",
        "Negative correlation means one variable increases while the other decreases.\n",
        "\n",
        "In feature engineering, correlation helps identify relationships between features and the target, guiding feature selection and reducing redundancy."
      ],
      "metadata": {
        "id": "W3id773SXyku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13) What does negative correlation mean?\n",
        "\n",
        "Negative correlation means that as one variable increases, the other decreases. It indicates an inverse relationship between the two variables. For example, as temperature rises, heating costs might decrease, showing a negative correlation."
      ],
      "metadata": {
        "id": "1Qr5eVrNXyh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14) How can you find correlation between variables in Python?\n",
        "\n",
        "We can find the correlation between variables in Python using the corr() method from Pandas:\n",
        "\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Display correlation matrix\n",
        "print(correlation_matrix)\n",
        "\n",
        "```\n",
        "This will show the correlation between all numeric variables in the dataset. For specific pairs, you can access them directly:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "correlation = df['column1'].corr(df['column2'])\n",
        "print(correlation)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9sAw3-aSaUjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15) What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Causation refers to a direct cause-and-effect relationship between two variables, where one variable directly influences the other.\n",
        "\n",
        "Difference Between Correlation and Causation:\n",
        "Correlation: Indicates that two variables are related in some way, but it doesn’t mean one causes the other. The relationship could be coincidental, or influenced by other factors.\n",
        "\n",
        "Example: Ice cream sales and drowning incidents might be correlated. As ice cream sales increase, so do drowning incidents. However, eating ice cream doesn’t cause drowning—both are related to warmer weather.\n",
        "\n",
        "Causation: Implies that one variable directly causes the change in another variable.\n",
        "\n",
        "Example: Smoking causes lung cancer. Here, smoking is the direct cause of lung cancer, establishing a cause-and-effect relationship.\n",
        "\n",
        "In short, correlation shows association, while causation shows direct influence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ARgOWp_9aUh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16) What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "An optimizer is an algorithm used to minimize the loss function by adjusting the model's parameters during training.\n",
        "\n",
        "Types of Optimizers:\n",
        "Gradient Descent: Adjusts parameters based on the gradient of the loss function.\n",
        "\n",
        "Example: Linear regression.\n",
        "\n",
        "Momentum: Adds a fraction of previous updates to accelerate convergence.\n",
        "\n",
        "Example: Deep neural networks.\n",
        "\n",
        "AdaGrad: Adapts learning rate for each parameter based on historical gradients.\n",
        "\n",
        "Example: NLP tasks.\n",
        "\n",
        "RMSprop: Uses moving averages of squared gradients to improve stability.\n",
        "\n",
        "Example: RNNs.\n",
        "\n",
        "Adam: Combines momentum and RMSprop for adaptive learning rates.\n",
        "\n",
        "Example: Deep learning tasks.\n",
        "\n",
        "Adadelta: Extension of AdaGrad with limited past gradients.\n",
        "\n",
        "Example: Complex loss surfaces.\n",
        "\n",
        "Optimizers help in faster convergence and better performance of models."
      ],
      "metadata": {
        "id": "cwmnNiQDaUgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17) What is sklearn.linear_model ?\n",
        "\n",
        "sklearn.linear_model is a module in Scikit-learn that provides a collection of linear models for both regression and classification tasks. These models assume a linear relationship between the input variables (features) and the target variable.\n",
        "\n",
        "Common Linear Models in sklearn.linear_model:\n",
        "Linear Regression (LinearRegression):\n",
        "\n",
        "Used for regression tasks where the target variable is continuous.\n",
        "\n",
        "Example: Predicting house prices based on features like size, location, etc.\n",
        "\n",
        "Logistic Regression (LogisticRegression):\n",
        "\n",
        "Used for binary classification tasks (predicts probabilities of classes).\n",
        "\n",
        "Example: Predicting whether an email is spam or not.\n",
        "\n",
        "Ridge Regression (Ridge):\n",
        "\n",
        "A variant of linear regression with L2 regularization to prevent overfitting.\n",
        "\n",
        "Example: When the data has multicollinearity or many features.\n",
        "\n",
        "Lasso Regression (Lasso):\n",
        "\n",
        "A variant of linear regression with L1 regularization that can also perform feature selection by shrinking some coefficients to zero.\n",
        "\n",
        "Example: When you want to select important features automatically.\n",
        "\n",
        "ElasticNet (ElasticNet):\n",
        "\n",
        "Combines L1 and L2 regularization, balancing the features of both Ridge and Lasso.\n",
        "\n",
        "Example: When you need both regularization and feature selection.\n",
        "\n",
        "Passive-Aggressive Classifier (PassiveAggressiveClassifier):\n",
        "\n",
        "A classifier that is useful for large-scale learning, which adapts to the data as it comes.\n",
        "\n",
        "Example: Real-time learning tasks, like spam filtering."
      ],
      "metadata": {
        "id": "yz3a1ZubaUbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18) What does model.fit() do? What arguments must be given?\n",
        "\n",
        "model.fit() trains a machine learning model using the provided training data. It adjusts the model's parameters based on the input features (X) and target values (y).\n",
        "\n",
        "Arguments:\n",
        "X: Input data (features) of shape (n_samples, n_features).\n",
        "\n",
        "y: Target data (labels/values) of shape (n_samples,).\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model.fit(X, y)\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Iq4DEJBWaUXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19) What does model.predict() do? What arguments must be given?\n",
        "\n",
        "model.predict() is a method used to make predictions using a trained model. Once the model is trained using model.fit(), you can use model.predict() to predict the target values for new, unseen data.\n",
        "\n",
        "What does model.predict() do?\n",
        "It uses the trained model to generate predictions for the input data.\n",
        "\n",
        "The model applies the learned parameters (from the fit() method) to the input features to make predictions.\n",
        "\n",
        "Required Argument:\n",
        "X: Input data (features) for prediction, typically in the same format as the training data (shape (n_samples, n_features)).\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Assuming the model is already trained with model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)  # X_test is the new input data\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jgfPFaFLaUVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20) What are continuous and categorical variables?\n",
        "\n",
        "\n",
        "Continuous Variables:\n",
        "Definition: These are variables that can take any value within a given range and are typically measured on a scale.\n",
        "\n",
        "Examples: Height, weight, temperature, age, income.\n",
        "\n",
        "Characteristics: They have infinite possible values within a range (e.g., a person's height could be 5.6, 5.62, or 5.625 feet).\n",
        "\n",
        "Categorical Variables:\n",
        "Definition: These are variables that take on a limited, fixed number of possible values or categories.\n",
        "\n",
        "Examples: Gender, country, marital status, type of car (Sedan, SUV, etc.).\n",
        "\n",
        "Characteristics: They represent different groups or classes, which can be either:\n",
        "\n",
        "Nominal: No inherent order (e.g., color, city).\n",
        "\n",
        "Ordinal: Have a meaningful order (e.g., education level: High School < Bachelor's < Master's).\n",
        "\n",
        "In summary:\n",
        "\n",
        "Continuous: Numerical, with infinite possible values.\n",
        "\n",
        "Categorical: Non-numeric, with distinct categories or groups."
      ],
      "metadata": {
        "id": "jZrWcxSiaUTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21) What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Feature Scaling:\n",
        "Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in a dataset. It ensures that all features contribute equally to the model, preventing features with larger ranges from dominating the learning process.\n",
        "\n",
        "Types of Feature Scaling:\n",
        "Normalization (Min-Max Scaling): Scales the data to a fixed range, usually [0, 1].\n",
        "\n",
        "Formula:\n",
        "X\n",
        "scaled\n",
        "=\n",
        "X\n",
        "−\n",
        "min\n",
        "⁡\n",
        "(\n",
        "X\n",
        ")\n",
        "max\n",
        "⁡\n",
        "(\n",
        "X\n",
        ")\n",
        "−\n",
        "min\n",
        "⁡\n",
        "(\n",
        "X\n",
        ")\n",
        "X\n",
        "scaled\n",
        "​\n",
        " =\n",
        "max(X)−min(X)\n",
        "X−min(X)\n",
        "​\n",
        "\n",
        "\n",
        "Standardization (Z-score Scaling): Centers the data around 0 with a unit standard deviation.\n",
        "\n",
        "Formula:\n",
        "X\n",
        "scaled\n",
        "=\n",
        "X\n",
        "−\n",
        "𝜇\n",
        "𝜎\n",
        "X\n",
        "scaled\n",
        "​\n",
        " =\n",
        "σ\n",
        "X−μ\n",
        "​\n",
        "  where μ is the mean and σ is the standard deviation of the feature.\n",
        "\n",
        "Why Feature Scaling Helps:\n",
        "Improves Model Performance: Algorithms like KNN, SVM, and gradient-based methods (e.g., logistic regression) perform better with scaled features, as they rely on distance calculations or gradients.\n",
        "\n",
        "Speeds Up Convergence: For algorithms like gradient descent, scaled features help in faster convergence by ensuring that features are on a similar scale.\n",
        "\n",
        "Prevents Model Bias: Without scaling, features with larger numerical ranges may dominate model learning, causing biased or suboptimal performance."
      ],
      "metadata": {
        "id": "gNpqVsN0cW0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22) How do we perform scaling in Python?\n",
        "\n",
        "The two most common methods are:\n",
        "\n",
        "1. Standardization (Z-score scaling)\n",
        "Tool: StandardScaler\n",
        "Scales data to have mean = 0 and standard deviation = 1'\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "```\n",
        "2. Normalization (Min-Max scaling)\n",
        "Tool: MinMaxScaler\n",
        "Scales data to a fixed range, usually [0, 1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "```\n",
        "\n",
        "When to Apply:\n",
        "Before training models like KNN, SVM, Logistic Regression, Neural Networks.\n",
        "\n",
        "After splitting data into training and testing sets.\n",
        "\n"
      ],
      "metadata": {
        "id": "KHpDBg2bcWlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23) What is sklearn.preprocessing?\n",
        "\n",
        "sklearn.preprocessing is a module in Scikit-learn (a popular Python library for machine learning) that provides a variety of tools to prepare your data before feeding it into a machine learning model.\n",
        "\n",
        "Preprocessing is a crucial step in machine learning because raw data often needs to be cleaned, transformed, or scaled to improve model performance.\n"
      ],
      "metadata": {
        "id": "_xGpNvhpcWUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24) How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "In Python, you can split data into training and testing sets using train_test_split from sklearn.model_selection.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "```\n",
        "Key Parameters:\n",
        "X: Features\n",
        "\n",
        "y: Target labels\n",
        "\n",
        "test_size=0.2: 20% test, 80% train\n",
        "\n",
        "random_state=42: Ensures reproducibility\n"
      ],
      "metadata": {
        "id": "Jv9EkRjzcV2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25) Explain data encoding?\n",
        "\n",
        "Data encoding converts categorical values into numbers so ML models can process them.\n",
        "\n",
        "**Common Types:**\n",
        "\n",
        "Label Encoding: Category → Integer (e.g., Red → 0, Blue → 1)\n",
        "\n",
        "One-Hot Encoding: Category → Binary columns (e.g., Red → [1, 0, 0])\n",
        "\n",
        "Ordinal Encoding: Category → Ordered integers (e.g., Low → 0, Medium → 1, High → 2)\n",
        "\n",
        "Use:\n",
        "\n",
        "One-Hot for unordered categories (nominal)\n",
        "\n",
        "Label/Ordinal for ordered categories (ordinal)"
      ],
      "metadata": {
        "id": "OMvo4f4BaUOs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbOHBbwyXmab"
      },
      "outputs": [],
      "source": []
    }
  ]
}